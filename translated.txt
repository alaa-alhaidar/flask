Der Anstieg des täglichen Datenvolumens, der Anstieg von datengetriebenen Geschäftsmodellen wie eHealth oder Kundenverhaltensanalyse und zuletzt IoT haben die Nutzer dazu veranlasst, ihre analytischen Aufgaben in Cloud-Diensten zu bearbeiten. Diese Big Data können bis zu Petabytes reichen, was den Prozess hinsichtlich Kosten- und Zeitersparnis schwieriger macht. Cloud-Nutzer stehen jedoch vor einer Herausforderung bei der Bereitstellung von Cloud-Ressourcen. Auf der einen Seite laden Cloud-Dienste Nutzer auf Basis der von ihnen gemieteten Ressourcen und der Zeit, die sie nutzen, auf der anderen Seite.cloud-Benutzer zielen in der Regel darauf ab, die Kosten zu minimieren, indem sie Ressourcen auf der Grundlage ihrer Einschränkungen bereitstellen. Bei Fehlallokation müssen die Nutzer jedoch möglicherweise für untergenutzte Ressourcen bezahlen oder zusätzliche Latenz bei der Fertigstellung von Arbeitsplätzen erleben. Daher ist es wichtig, dass Cloud-Benutzer eine Schätzung der Ausführungszeit vor der Bereitstellung eines Clusters haben. Dies hilft, eine Unternutzung von Ressourcen zu vermeiden, Kosten zu sparen und eine ordnungsgemäße Arbeitsplanung zu ermöglichen.Der zweite Ansatz zielt darauf ab, die Kosten des Modelltrainings durch einen analytischen Ansatz zu minimieren, um die Laufzeit auf der Basis von Clustern vorherzusagen, und zwar mit Hilfe von überwachten Algorithmen wie Gradient Boosted Decision Tree (GBDT), Random Forest Regression (RFR) und Multi-Linear Regression (MLR).Im analytischen Ansatz reduziert diese Arbeit die Modellkosten um 61\% von 234 auf 90 Runs im Vergleich zu unserem ML-Ansatz und erreicht einen niedrigeren durchschnittlichen absoluten Fehler von weniger als 20\% in beiden Ansätzen. Diese Arbeit erreicht einen niedrigeren durchschnittlichen absoluten Fehler von 11,247\% im analytischen Ansatz im Vergleich zum hochmodernen ERNEST-Modell von 17,871\%. Darüber hinaus reduziert diese Arbeit die Kosten bestimmter Spark-Jobs um bis zu 61\% im Vergleich zu static-Node-Zuweisungen mit unserem Gesamt-Node-Empfehlungsalgorithmus für Spark-Jobs bei einer Zielzeit.